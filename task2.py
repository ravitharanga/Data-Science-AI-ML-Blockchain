# -*- coding: utf-8 -*-
"""task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mvx5GluftnrghQ7Njgk5FheYeKn6PilF
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

"""# from sklearn.ensemble import RandomForestClassifier = is for discrete numbers (labels) not for float values"""

df = pd.read_csv('googleplaystore.csv')

# Display first five rows

df.head()

# display last 5 rows

df.tail()

# display information about the dataset

df.info()

# checking data types of each column

df.dtypes

# display main mathematical charachteristics

df.describe()

df[df.columns[-1]]

# find categorical variables

categorical = [var for var in df.columns if df[var].dtype=='O']

print('There are {} categorical variables\n'.format(len(categorical)))

print('The categorical variables are :\n\n', categorical)

# view the categorical variables

df[categorical].head()

# view null values total in categorical vars

df[categorical].isnull().sum()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt # for data visualization purposes
import seaborn as sns # for statistical data visualization
# %matplotlib inline


# view frequency distribution of categorical variables

for var in categorical:

    print(df[var].value_counts()/float(len(df)))

# check labels in Type variable

df['Type'].unique()

# check labels in Content Rating variable

df['Content Rating'].unique()

# check labels in Current Ver variable

df['Current Ver'].unique()

# check labels in Android Ver variable

df['Android Ver'].unique()

# displaying all columns / features & target var

df.columns

# each class how many records (Diagnosis)

df.value_counts()

# drop unwanted columns

df. drop('Unnamed: 0', axis=1, inplace=True)

# drop unwanted columns

df. drop('Size', axis=1, inplace=True)

# displaying all columns once again to check existing columns after dropping unnecessary columns

df.columns

# Finding rows where 'Installs' cannot be converted to numeric
invalid_installs = df['Installs'].str.contains('^[a-zA-Z]', regex=True)
invalid_installs_rows = df[invalid_installs]

# Dropping these rows
df_cleaned = df[~invalid_installs]

# Try to convert once again
df_cleaned['Installs'] = df_cleaned['Installs'].str.replace('+', '').str.replace(',', '').astype(float)

# Checking the data types
df_cleaned.dtypes

# cleaned data

df_cleaned.head()

# invalid data rows from the installs column

invalid_installs_rows

df_cleaned.head()

df_cleaned.dtypes

# Price column has $ sign hence before convert it to float, needs to remove the $ sign and then convert to float type

df_cleaned['Price_float'] = df_cleaned['Price'].str.replace('$', '').astype(float)

# Convert 'Last Updated' to datetime format
df_cleaned['Last Updated_dt'] = pd.to_datetime(df_cleaned['Last Updated'], errors='coerce', format='%d-%b-%y')

df_cleaned.head()

# Analysing Exploratory Data / EDA

# Checking for missing values
missing_values = df_cleaned.isnull().sum()
missing_values

# Statistical summary of numeric columns

statistical_summary = df_cleaned.describe()
statistical_summary

# apply the log transformation with NumPy

df_cleaned['Installs_log'] = df_cleaned['Installs'].apply(lambda x: np.log(x + 1))

# checking any null values

df_cleaned['Installs_log'].isnull().sum()

df_cleaned.columns

# set the aesthetic style of the plots

sns.set_style("whitegrid")

fig, ax = plt.subplots(2, 2, figsize=(10, 10), dpi=100)

# histogram of App Ratings
sns.histplot(df_cleaned['Rating'].dropna(), bins=20, kde=True, ax=ax[0, 0])  # Drop NA values for clean plotting
ax[0, 0].set_title('Distribution of App Ratings')
ax[0, 0].set_xlabel('Rating')
ax[0, 0].set_ylabel('Frequency')

# boxplot of app Ratings

fig, ax = plt.subplots(2, 2, figsize=(8, 8), dpi=100)

sns.boxplot(x='Rating', data=df_cleaned, ax=ax[0, 1])
ax[0, 1].set_title('Boxplot of App Ratings')
ax[0, 1].set_xlabel('Rating')


# histogram of App Installs (Using Log-Transformed Data)

sns.histplot(df_cleaned['Installs_log'], bins=20, kde=True, ax=ax[1, 0])
ax[1, 0].set_title('Distribution of App Installs (Log Scale)')
ax[1, 0].set_xlabel('Installs (Log Scale)')
ax[1, 0].set_ylabel('Frequency')

# Boxplot of App Prices (Adjusting for visualization)

sns.boxplot(x='Price_float', data=df_cleaned, ax=ax[1, 1])
ax[1, 1].set_title('Boxplot of App Prices')
ax[1, 1].set_xlabel('Price')
ax[1, 1].set_xlim(0, df_cleaned['Price_float'].quantile(0.95))  # Limiting x-axis to 95th percentile for better visualization

plt.tight_layout()
plt.show()

# view null values total in categorical vars

categorical = [var for var in df_cleaned.columns if df_cleaned[var].dtype=='O']

df_cleaned[categorical].isnull().sum()

# finding unique values in Type column

df_cleaned['Type'].unique()

# above same result can be generated using groupby function
# find number of instances (rows) that belong to 'Type' feature/variable (absolute count)

df_cleaned.groupby('Type').size()

# using map function to encode categorical variable values into numeric values

# Define a mapping dictionary
mapping_dict = {'Free': 0, 'Paid': 1}

# Mapping the values in the 'Type' column using the dictionary
df_cleaned['Type_Encoded'] = df_cleaned['Type'].map(mapping_dict)

# visualizing App Types in a pie chart

plt.figure(figsize=(5,5))
fig = df_cleaned.groupby("Type")["Type"].count().plot(kind="pie", autopct='%1.0f%%', shadow=False, explode=(0,0.1))
fig.set_title("App Types")
fig.set_ylabel("")
plt.show()

# Generating a sns line plot to compare ratings vs paid apps

plt.figure(figsize=(10,5))
sns.lineplot(data=df_cleaned[df_cleaned["Type"] == "Paid"], x="Price_float", y="Rating")
plt.title("Distribution of Ratings of Paid Apps")
plt.show()

# finding unique values in Content Rating column

df_cleaned['Content Rating'].unique()

# count 'Content Rating' column's classes

df_cleaned['Content Rating'].value_counts()

# visualizing the distribution of Content Rating

plt.figure(figsize=(10,5))
sns.countplot(x=df_cleaned["Content Rating"], palette="YlGnBu_r")
plt.title("Content Ratings")
plt.show()

# using map function to encode categorical variable values into numeric values

# Define a mapping dictionary
mapping_dict = {'Everyone': 1, 'Teen':2, 'Everyone 10+' :3, 'Mature 17+' : 4, 'Adults only 18+' : 5, 'Unrated' : 0}

# Mapping the values in the 'Type' column using the dictionary
df_cleaned['Content Rating_Encoded'] = df_cleaned['Content Rating'].map(mapping_dict)



# finding unique values in Genres column

df_cleaned['Genres'].unique()

# count 'Genres' column's classes

df_cleaned['Genres'].value_counts()

# using sklearn to encode categorical variable values into numeric values

from sklearn.preprocessing import LabelEncoder

# Creating a LabelEncoder object
labelEncoderObj = LabelEncoder()

# Genres column has many uniques values / classes hence auto encoder is the best method instead of map

# Fit and transforming the categorical variable to numeric values
df_cleaned['Genres_Encoded'] = labelEncoderObj.fit_transform(df_cleaned['Genres'])

# finding unique values in the Category column

df_cleaned['Category'].unique()

# count Category column classes

df_cleaned['Category'].value_counts()

# Encoding Category column values to numeric values

# Fit and transforming the categorical variable to numeric values
df_cleaned['Category_Encoded'] = labelEncoderObj.fit_transform(df_cleaned['Category'])

# count App column classes

df_cleaned['App'].value_counts()

# labels in 'App' variable

df_cleaned.App.unique()



# Encoding App column values to numeric values

# Fit and transforming the categorical variable to numeric values
df_cleaned['App_Encoded'] = labelEncoderObj.fit_transform(df_cleaned['App'])

# labels in 'Current Ver' variable

df_cleaned['Current Ver'].unique()

# Encoding Current Ver column values to numeric values

# Fit and transforming the categorical variable to numeric values
df_cleaned['Current Ver_Encoded'] = labelEncoderObj.fit_transform(df_cleaned['Current Ver'])

# labels in 'Android Ver' variable

df_cleaned['Android Ver'].unique()

# Encoding Android Ver column values to numeric values

# Fit and transforming the categorical variable to numeric values
df_cleaned['Android Ver_Encoded'] = labelEncoderObj.fit_transform(df_cleaned['Android Ver'])

# adding a new column by converting the Reviews column to a float type

# df_cleaned['Reviews_Log'] = df_cleaned['Reviews'].astype(float).apply(np.log) # THIS LOG VALUES THROW AN ERROR AT THE TIME OF FIT THE MODEL

df_cleaned['Reviews_float'] = df_cleaned['Reviews'].astype(float)

# visualization setup
fig, ax = plt.subplots(2, 2, figsize=(10, 10), dpi=100)


# best categories
top_categories = df_cleaned['Category'].value_counts().head(10)
sns.barplot(x=top_categories.values, y=top_categories.index, ax=ax[0, 0])
ax[0, 0].set_title('Top 10 App Categories')
ax[0, 0].set_xlabel('Number of Apps')
ax[0, 0].set_ylabel('Category')


# common android versions
common_android_versions = df_cleaned['Android Ver'].value_counts().head(10)
sns.barplot(x=common_android_versions.values, y=common_android_versions.index, ax=ax[0, 1])
ax[0, 1].set_title('Top 10 Common Android Versions Required')
ax[0, 1].set_xlabel('Number of Apps')
ax[0, 1].set_ylabel('Android Version')


# free vs. paid apps distribution
free_vs_paid = df_cleaned['Type'].value_counts()
sns.barplot(x=free_vs_paid.index, y=free_vs_paid.values, ax=ax[1, 0])
ax[1, 0].set_title('Distribution of Free vs. Paid Apps')
ax[1, 0].set_xlabel('Type')
ax[1, 0].set_ylabel('Number of Apps')


# install ranges per category - mean installs
mean_installs_per_category = df_cleaned.groupby('Category')['Installs'].mean().sort_values(ascending=False).head(10)
sns.barplot(x=mean_installs_per_category.values, y=mean_installs_per_category.index, ax=ax[1, 1])
ax[1, 1].set_title('Top 10 Categories by Mean Installs')
ax[1, 1].set_xlabel('Mean Installs')
ax[1, 1].set_ylabel('Category')


plt.tight_layout()
plt.show()

fig, ax = plt.subplots(2, 2, figsize=(8, 8), dpi=90)

# rating vs. reviews
sns.scatterplot(x='Reviews', y='Rating', data=df_cleaned, ax=ax[0, 0], alpha=0.5)
ax[0, 0].set_title('Rating vs. Reviews')
ax[0, 0].set_xlabel('Reviews')
ax[0, 0].set_ylabel('Rating')
ax[0, 0].set_xscale('log')  # Using log scale due to wide range of values

# rating vs. installs
sns.scatterplot(x='Installs', y='Rating', data=df_cleaned, ax=ax[0, 1], alpha=0.5)
ax[0, 1].set_title('Rating vs. Installs')
ax[0, 1].set_xlabel('Installs')
ax[0, 1].set_ylabel('Rating')
ax[0, 1].set_xscale('log')  # Using log scale due to wide range of values

# price vs. installs
sns.scatterplot(x='Price', y='Installs', data=df_cleaned, ax=ax[1, 0], alpha=0.5)
ax[1, 0].set_title('Price vs. Installs')
ax[1, 0].set_xlabel('Price')
ax[1, 0].set_ylabel('Installs')
ax[1, 0].set_xscale('log')  # Using log scale due to wide range of values
ax[1, 0].set_yscale('log')  # Also log scale for better visualization

# reviews vs. installs
sns.scatterplot(x='Reviews', y='Installs', data=df_cleaned, ax=ax[1, 1], alpha=0.5)
ax[1, 1].set_title('Reviews vs. Installs')
ax[1, 1].set_xlabel('Reviews')
ax[1, 1].set_ylabel('Installs')
ax[1, 1].set_xscale('log')  # Using log scale due to wide range of values
ax[1, 1].set_yscale('log')  # Also log scale for better visualization

plt.tight_layout()
plt.show()

df_cleaned.columns

# sorting the dataframe by the 'reviews' column and select the top 10 rows
top_10_apps = df_cleaned.sort_values('Reviews_float', ascending=False).head(10)

# creating a bar plot
plt.figure(figsize=(5, 5))
sns.barplot(data=top_10_apps, x='Reviews_float', y='App', palette='viridis')
plt.xlabel('Number of Reviews')
plt.ylabel('App')
plt.title('Top 4 Most Reviewed Apps')

plt.show()

# Plotting a pie chart to display the distribution of price.

downloads = df_cleaned['Price'].value_counts()
plt.figure(figsize=(5,5))
downloads.plot(kind='pie', autopct='%1.1f%%')
plt.title("Distribution based on Price")
plt.ylabel('')
plt.show()

# Rating column has many missing values
# it is possible to use a ML model to predict missing categorical values based on other features in the dataset.

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='most_frequent')
df_cleaned['Rating_mis_handled'] = imputer.fit_transform(df_cleaned[['Rating']])

df_cleaned['Rating_mis_handled'].unique()

# Generating a box plot to find outliers

plt.boxplot(df_cleaned['Rating_mis_handled'], vert=False, labels=['Data'])
plt.title('Box Plot of Data with Outliers')
plt.xlabel('Values')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# plotting the destribution of the column data to find the skewness

plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
sns.distplot(df_cleaned['Rating_mis_handled'])

plt.show()

# finding the skewness
# negative value means the distribution of the column's data has been skewed to the left

df_cleaned['Rating_mis_handled'].skew()

# finding the bounadray values

print('Heigth allowed=',df_cleaned['Rating_mis_handled'].mean()+3*df_cleaned['Rating_mis_handled'].std())
print('Lowest allowed=',df_cleaned['Rating_mis_handled'].mean()-3*df_cleaned['Rating_mis_handled'].std())

# finding outliers

df_cleaned[(df_cleaned['Rating_mis_handled']>5.67 )|( df_cleaned['Rating_mis_handled']<2.77)]

# checking the dimention of the data set

df_cleaned.shape

# trimming

df_cleaned=df_cleaned[(df_cleaned['Rating_mis_handled']<5.31) & (df_cleaned['Rating_mis_handled']>3.28)]

# checking the dimention of the data set after trimming / removing outliers

df_cleaned.shape

"""UNIT TEST / REMOVING OUTLIERS"""

# UNIT TEST

# defining a function to remove outliers


import numpy as np

def remove_outliers(data):
    # Calculate Q1 and Q3
    q1 = np.percentile(data, 25)
    q3 = np.percentile(data, 75)

    # Calculate IQR
    iqr = q3 - q1

    # Define threshold multiplier (1.5 is a common choice)
    threshold = 1.5

    # Identify outliers
    lower_bound = q1 - threshold * iqr
    upper_bound = q3 + threshold * iqr

    outliers = (data < lower_bound) | (data > upper_bound)

    # Remove outliers
    cleaned_data = data[~outliers]

    return cleaned_data


cleaned_data = remove_outliers(df_cleaned['Rating_mis_handled'])

print("Original Data:", df_cleaned['Rating_mis_handled'])
print("Cleaned Data:", cleaned_data)



# generating Z score

from scipy import stats
import numpy as np

z = np.abs(stats.zscore(df_cleaned['Rating_mis_handled']))
print(z)



# finding row index of outliers

# based on the z-score criteria, the below code prints positions of the outliers in the column
# threshold = 2

# Position of the outlier
print(np.where(z > 2))




# dropping rows that are having outlier values

rows_to_drop = np.where(z > 2)
print(rows_to_drop)
# converting row indexes to a 1D array
rows_to_drop = np.unique(rows_to_drop[0])

# Using a for loop to drop rows
for row_index in rows_to_drop:
    df_cleaned = df_cleaned.drop(index=row_index)

# dropping rows based on the array of row indexes
# df_cleaned.drop(index=rows_to_drop)

# Generating a box plot to after removing outliers

plt.boxplot(df_cleaned['Rating_mis_handled'], vert=False, labels=['Data'])
plt.title('Box Plot of Data with Outliers')
plt.xlabel('Values')
plt.show()

"""REMOVE OUTLIERS FROM THE PRICE_FLOAT"""

df_cleaned.columns

# Generating a box plot to after removing outliers

plt.boxplot(df_cleaned['Price_float'], vert=False, labels=['Data'])
plt.title('Box Plot of Data with Outliers')
plt.xlabel('Values')
plt.show()

# before trimming

df_cleaned.Price_float.shape

# find skew value

df_cleaned['Price_float'].skew()

# Finding the bounadray values

print('Heigth allowed=',df_cleaned['Price_float'].mean()+3*df_cleaned['Price_float'].std())
print('Lowest allowed=',df_cleaned['Price_float'].mean()-3*df_cleaned['Price_float'].std())

#find the outlier

df_cleaned[(df_cleaned['Price_float']>2.52 )|( df_cleaned['Price_float']<-2.16)]

# trimming

df_cleaned=df[(df_cleaned['Price_float']<2.52) & (df_cleaned['Price_float']> -2.16)]

# Generating a box plot to after removing outliers

plt.boxplot(df_cleaned['Price_float'], vert=False, labels=['Data'])
plt.title('Box Plot of Data with Outliers')
plt.xlabel('Values')
plt.show()

# numerical variables

numerical = [var for var in df_cleaned.columns if df_cleaned[var].dtype!='O']

print('There are {} numerical variables\n'.format(len(numerical)))

print('numerical vars :', numerical)

df_cleaned.columns

df_cleaned.info()

# viewing data types

df_cleaned.dtypes

pip install --upgrade category_encoders

# missing values in numerical variable

df_cleaned[numerical].isnull().sum()

# numerical variable / target variable / dependent variable

df_cleaned[categorical].head()

# numerical variable / target variable / dependent variable

df_cleaned[numerical].head()

# missing values in numerical variable

df_cleaned[numerical].isnull().sum()

# viewing total null of each column

df_cleaned.isnull().sum()

# check na values

import numpy as np

np.isnan(df_cleaned['Type_Encoded']).sum()

# drop NA / NULL values
# df_cleaned = df_cleaned['Type_Encoded'].dropna() NO NEED THIS

# viewing total null of each column

df_cleaned.isnull().sum()

df_cleaned['Type_Encoded'].unique()

df_cleaned['Type_Encoded'].replace(np.NaN, 0, inplace=True)

# viewing total null of each column

df_cleaned.isnull().sum()



# using sklearn to encode categorical variable values into numeric values

# from sklearn.preprocessing import LabelEncoder

# Creating a LabelEncoder object
# labelEncoderObj = LabelEncoder()

# Fit and transforming the categorical variable to numeric values
# df_cleaned['Installs_Encoded'] = labelEncoderObj.fit_transform(df_cleaned['Installs']) // NO NEED THIS, I MADE THE LOG VALUE











df_cleaned.columns







"""Removing duplicates from App Encoded column"""

# finding duplicates

duplicates_count = df_cleaned.duplicated(subset=['App_Encoded']).sum()

print("Number of duplicate rows:", duplicates_count)

# Removing duplicates

df_cleaned.drop_duplicates(subset=['App_Encoded'], inplace=True)

print("Number of duplicate rows:", df_cleaned.duplicated().sum())

df_cleaned.columns

# creating an iteractive scatter plot to display reviews vs rating

import plotly.express as px

fig = px.scatter(df_cleaned, x='Rating', y='Reviews_float', hover_data=['App_Encoded', 'Category_Encoded', 'Installs_log'])

# marker size, shape, and border line customization
fig.update_traces(
    marker=dict(
        size=12,  # to increase the size of the data points
        symbol='diamond',  # to change the shape of the data points to a circle
        line=dict(
            color='orange',  # border line color
            width=2  # width of the border line
        ),
        color='rgba(0, 0, 0, 0)'  # marker color

    )
)

# Customize the layout
fig.update_layout(
    title="Reviews vs Ratings in Google Apps Store",
    xaxis_title="Ratings",
    yaxis_title="Number of Reviews",
    font=dict(
        family="Arial, sans-serif",
        size=12,
        color="blue"
    ),
    plot_bgcolor='black',  # plot background color
    hoverlabel=dict(
        bgcolor="yellow",  # hover label background color
        font_size=14
    ),
    xaxis=dict(
        gridcolor='lightgrey'  # gridlines
    ),
    yaxis=dict(
        gridcolor='lightgrey'  # gridlines
    )
)

fig.show()

df_cleaned.columns



# viewing total null of each column

df_cleaned.isnull().sum()

"""SPLIT DATA TO TRAIN AND TEST"""

# Spliting data into two arrays: X (features) and y (labels).

feature_columns = ['Type_Encoded', 'Content Rating_Encoded', 'Genres_Encoded','Category_Encoded', 'App_Encoded', 'Reviews_float']

x = df_cleaned[feature_columns].values
y = df_cleaned['Rating_mis_handled'].values

# Split data into 80% of train data set and 20% of test data set

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=40)

# checking test and train set sizes

print(X_train.shape,
X_test.shape,
y_train.shape,
y_test.shape)

# Standardize features by removing the mean and scaling to unit variance
# Standard Scaling for model efficiency

from sklearn.preprocessing import StandardScaler

sc = StandardScaler(with_std  = True ,with_mean = True, copy = True)
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Split data into 80% of train data set and 20% of test data set

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=40)

# checking test and train set sizes

print(X_train.shape,
X_test.shape,
y_train.shape,
y_test.shape)

# Standardize features by removing the mean and scaling to unit variance
# Standard Scaling for model efficiency

from sklearn.preprocessing import StandardScaler

sc = StandardScaler(with_std  = True ,with_mean = True, copy = True)
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.ensemble import RandomForestRegressor
# RandomForestRegressor is a classifier which expects float numbers (labels), but RandomForestClassifier expects discrete numbers (labels).

# Create another Random Forest Regressor model with a different parameters
rfr_model = RandomForestRegressor(n_estimators=100, random_state=52)

# Build a forest of trees from the training set (X, y).
# Fit the model on the training data
# Fit the model with X_Train

rfr_model.fit(X_train, y_train)

# Assigning predictions that has captured from the features of the test set
# Make predictions on the test set

y_pred = rfr_model.predict(X_test)

# Evaluating the model

# thisis to match y_test values that has been captured earlier with the y_pred predicted values
# accuracy score measures the proportion of the correct predictions out of the total predictions
# accuracy = accuracy_score(y_test, y_pred) : accuracy_score function = this is for classification tasks only, therefore, score() function can be used as follow

rfr_model.score(X_train, y_train)

# Evaluating the model

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Display the regression report
print(f'Mean Absolute Error (MAE): {mae}')
print(f'Mean Squared Error (MSE): {mse}')
print(f'R-squared (R2): {r2}') # # In regression tasks, it is good to use the metric R-squared (co-effecient of determination)

# Visualize predicted vs actual values
plt.scatter(y_test, y_pred)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values for Random Forest Regressor')
plt.show()







"""TRYING TO INCREASE MODEL'S PERFORMANCE"""

# Spliting data into two arrays: X (features) and y (labels).

feature_columns = ['Installs_log','Type_Encoded', 'Content Rating_Encoded', 'Genres_Encoded','Category_Encoded', 'App_Encoded', 'Current Ver_Encoded',
       'Android Ver_Encoded','Reviews_float']

x = df_cleaned[feature_columns].values
y = df_cleaned['Rating_mis_handled'].values

# Split data into 80% of train data set and 20% of test data set

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=40)

# checking test and train set sizes

print(X_train.shape,
X_test.shape,
y_train.shape,
y_test.shape)

# Standardize features by removing the mean and scaling to unit variance
# Standard Scaling for model efficiency

from sklearn.preprocessing import StandardScaler

sc = StandardScaler(with_std  = True ,with_mean = True, copy = True)
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Split data into 80% of train data set and 20% of test data set

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=40)

# RandomForestRegressor is a classifier which expects float numbers (labels), but RandomForestClassifier expects discrete numbers (labels).

# Create another Random Forest Regressor model with a different parameters
rfr_model = RandomForestRegressor(n_estimators=100, random_state=52)

# Build a forest of trees from the training set (X, y).
# Fit the model on the training data
# Fit the model with X_Train

rfr_model.fit(X_train, y_train)

# Assigning predictions that has captured from the features of the test set
# Make predictions on the test set

y_pred = rfr_model.predict(X_test)

# Evaluating the model

# thisis to match y_test values that has been captured earlier with the y_pred predicted values
# accuracy score measures the proportion of the correct predictions out of the total predictions
# accuracy = accuracy_score(y_test, y_pred) : accuracy_score function = this is for classification tasks only, therefore, score() function can be used as follow

rfr_model.score(X_train, y_train)

# Evaluating the model

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Display the regression report
print(f'Mean Absolute Error (MAE): {mae}')
print(f'Mean Squared Error (MSE): {mse}')
print(f'R-squared (R2): {r2}') # # In regression tasks, it is good to use the metric R-squared (co-effecient of determination)

# Visualize predicted vs actual values
plt.scatter(y_test, y_pred)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values for Random Forest Regressor')
plt.show()















# DONT RUN FEATURE SCALLING HERE

# feature scalling

# from sklearn.preprocessing import RobustScaler

# scaler = RobustScaler()

# X_train = scaler.fit_transform(X_train)

# X_test = scaler.transform(X_test)

# X_train = pd.DataFrame(X_train)

# X_test = pd.DataFrame(X_test)

# X_train.head()

# X_test.head()









# For finding best features among others.
# To label the features form best to worst using above defined feature_columns
# Using RandomForestRegressor

from sklearn.ensemble import RandomForestRegressor

r_forest_r = RandomForestRegressor(n_estimators=500,random_state=1)
r_forest_r.fit(X_train, y_train)

importances = r_forest_r.feature_importances_

indices = np.argsort(importances)[::-1]

for f in range(X_train.shape[1]):
    print("%2d) %-*s %f" % (f + 1, 30, feature_columns[indices[f]], importances[indices[f]]))



plt.title('Feature Importance')
plt.bar(range(X_train.shape[1]), importances[indices], align='center')

# plt.xticks(range(X_train.shape[1]), feature_columns[indices], rotation=90)
plt.tick_params(axis='x', labels=feature_columns[indices], rotation=90)
plt.xlim([-1, X_train.shape[1]])
plt.tight_layout()

plt.show()

# displaying predictions and actual values with their differences

for z in zip(y_test, y_pred):
    print(z, (z[0]-z[1]) /z[0] )

# plotting actual Ratings (orange line) and Predicted Ratings(blue line)

r = []
for pair in  zip(y_pred, y_test):
  r.append(pair)

plt.plot(r)

# Effect of estimators on score
# With np.range it is possible to find the most efficient estimator value.

estimators = np.arange(5, 2500, 250) # 0 to 2500 increased with 250
scores = []
for n in estimators:
    rfr_model.set_params(n_estimators=n)
    rfr_model.fit(X_train, y_train)
    scores.append(rfr_model.score(X_train, y_train))
plt.title("Effect of n_estimators")
plt.xlabel("n_estimator")
plt.ylabel("score")
plt.plot(estimators, scores)



stimators = np.arange(5, 2500, 250) # 5 to 2500 increased with 250
scores = []
for n in estimators:
    rfr_model.set_params(n_estimators=n)
    rfr_model.fit(X_train, y_train)
    scores.append(rfr_model.score(X_test, y_test))
plt.title("Effect of n_estimators")
plt.xlabel("n_estimator")
plt.ylabel("score")
plt.plot(estimators, scores)

y_pred[:20]

y_pred_test = r_forest_r.predict(X_test)
metrics.score(y_true, y_pred_test)















"""UNIT TEST"""

# normalizing data / unit test

from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Sample dataset
data = {'Column1': [10, 20, 30, 40, 50]}
df = pd.DataFrame(data)

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Fit and transform the data
df['Column1_normalized'] = scaler.fit_transform(df[['Column1']])

# Display the original and normalized data
print("Original Data:")
print(df[['Column1']])

print("\nNormalized Data:")
print(df[['Column1_normalized']])

